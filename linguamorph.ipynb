{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff53df9c",
   "metadata": {},
   "source": [
    "# LINGUAMORPH\n",
    "\n",
    "The software in this notebook is intended to take in text and output variations in sound and stress.\n",
    "\n",
    "Potential applications include:\n",
    "\n",
    "  - homophone phrases (variations on a phrase that sound the same or similar)\n",
    "  - extreme alliteration\n",
    "  - spoonerism-style scrambles\n",
    "  - homophone anagrams\n",
    "  - homophone loops\n",
    "  - shrinking and expanding text\n",
    "  - two different stories that sound the same\n",
    "  - text that morphs to text with meaningful intermediates\n",
    "  - stress-rhythm-seeded text\n",
    "  - faux/foe translations\n",
    "  - rhyming/rapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51f1a6",
   "metadata": {},
   "source": [
    "## Phonemes and Carnegie Mellon Pronouncing Dictionary\n",
    "\n",
    "(See https://github.com/cmusphinx/cmudict/tree/4c6a365cea2c34340ffc218d5af7a38920fa7e37)\n",
    "\n",
    "From https://www.nltk.org/_modules/nltk/corpus/reader/cmudict.html:\n",
    "\n",
    "The Carnegie Mellon Pronouncing Dictionary [cmudict.0.6]\n",
    "Copyright 1998 Carnegie Mellon University\n",
    "\n",
    "File Format: Each line consists of an uppercased word, a counter\n",
    "(for alternative pronunciations), and a transcription.  Vowels are\n",
    "marked for stress (1=primary, 2=secondary, 0=no stress).  E.g.:\n",
    "NATURAL 1 N AE1 CH ER0 AH0 L\n",
    "\n",
    "The dictionary contains 127069 entries.  Of these, 119400 words are assigned\n",
    "a unique pronunciation, 6830 words have two pronunciations, and 839 words have\n",
    "three or more pronunciations.  Many of these are fast-speech variants.\n",
    "\n",
    "Phonemes: There are 39 phonemes, as shown below:\n",
    "\n",
    "    Phoneme Example Translation    Phoneme Example Translation\n",
    "    ------- ------- -----------    ------- ------- -----------\n",
    "    AA      odd     AA D           AE      at      AE T\n",
    "    AH      hut     HH AH T        AO      ought   AO T\n",
    "    AW      cow     K AW           AY      hide    HH AY D\n",
    "    B       be      B IY           CH      cheese  CH IY Z\n",
    "    D       dee     D IY           DH      thee    DH IY\n",
    "    EH      Ed      EH D           ER      hurt    HH ER T\n",
    "    EY      ate     EY T           F       fee     F IY\n",
    "    G       green   G R IY N       HH      he      HH IY\n",
    "    IH      it      IH T           IY      eat     IY T\n",
    "    JH      gee     JH IY          K       key     K IY\n",
    "    L       lee     L IY           M       me      M IY\n",
    "    N       knee    N IY           NG      ping    P IH NG\n",
    "    OW      oat     OW T           OY      toy     T OY\n",
    "    P       pee     P IY           R       read    R IY D\n",
    "    S       sea     S IY           SH      she     SH IY\n",
    "    T       tea     T IY           TH      theta   TH EY T AH\n",
    "    UH      hood    HH UH D        UW      two     T UW\n",
    "    V       vee     V IY           W       we      W IY\n",
    "    Y       yield   Y IY L D       Z       zee     Z IY\n",
    "    ZH      seizure S IY ZH ER\n",
    "    \n",
    "From https://www.pythonstudio.us/language-processing/a-pronouncing-dictionary.html:\n",
    "\n",
    "For each word, this lexicon provides a list of phonetic codes—distinct labels for each contrastive sound—known as phones. Observe that fire has two pronunciations (in U.S. English): the one-syllable F AY1 R, and the two-syllable F AY1 ER0. The symbols in the CMU Pronouncing Dictionary are from the Arpabet, described in more detail at http://en.wikipedia.org/wiki/Arpabet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84caa8",
   "metadata": {},
   "source": [
    "## Code to prepare and filter CMU words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcefc283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "phoneme_list = ['AA','AH','AW','B','D','EH','EY','G','IH','JH','L','N','OW','P','S','T','UH','V','Y','ZH',\n",
    "                'AE','AO','AY','CH','DH','ER','F','HH','IY','K','M','NG','OY','R','SH','TH','UW','W','Z']\n",
    "vowel_list = ['AA','AH','AW','EH','EY','IH','OW','UH','AE','AO','AY','ER','IY','OY','UW']\n",
    "single_consonants = ['B','D','G','JH','L','N','P','S','T','V','Y','ZH','CH',\n",
    "                     'DH','F','HH','K','M','NG','R','SH','TH','W','Z']\n",
    "multiple_consonants = []\n",
    "for c1 in single_consonants:\n",
    "    for c2 in single_consonants:\n",
    "        if c2 != c1:\n",
    "            multiple_consonants.append(c1 + '+' + c2)\n",
    "            for c3 in single_consonants:\n",
    "                if c3 != c2:\n",
    "                    multiple_consonants.append(c1 + '+' + c2 + '+' + c3)\n",
    "                    for c4 in single_consonants:\n",
    "                        if c4 != c3:\n",
    "                            multiple_consonants.append(c1 + '+' + c2 + '+' + c3 + '+' + c4)\n",
    "                            #for c5 in single_consonants:\n",
    "                            #    if c5 != c4:\n",
    "                            #        multiple_consonants.append(c1 + '+' + c2 + '+' + c3 + '+' + c4 + '+' + c5)\n",
    "\n",
    "consonant_list = single_consonants + multiple_consonants\n",
    "#print(len(consonant_list))\n",
    "#print(consonant_list)\n",
    "\n",
    "\n",
    "def combine_consonants(phonemes):\n",
    "    '''\n",
    "    >>> combine_consonants(['Y','IY','L','D'])\n",
    "    ['Y', 'IY', 'L+D']    \n",
    "    '''\n",
    "    phonemes_with_combined_consonants = []\n",
    "    P = len(phonemes)\n",
    "    i = 0\n",
    "    while i < P:\n",
    "        loop = True\n",
    "        while loop:\n",
    "            p1 = phonemes[i]\n",
    "            i += 1\n",
    "            if p1 in single_consonants:\n",
    "                if P > i:\n",
    "                    p2 = phonemes[i]\n",
    "                    i += 1\n",
    "                    if p2 in single_consonants:\n",
    "                        if P > i:\n",
    "                            p3 = phonemes[i]\n",
    "                            i += 1\n",
    "                            if p3 in single_consonants:\n",
    "                                if P > i:\n",
    "                                    p4 = phonemes[i]\n",
    "                                    i += 1\n",
    "                                    if p4 in single_consonants:\n",
    "                                        phonemes_with_combined_consonants.append(p1 + '+' + p2 + '+' + p3 + '+' + p4)\n",
    "                                        #if P > i:\n",
    "                                        #    p5 = phonemes[i]\n",
    "                                        #    i += 1\n",
    "                                        #    if p5 in single_consonants:\n",
    "                                        #        phonemes_with_combined_consonants.append(p1 + '+' + p2 + '+' + p3 + '+' + p4 + '+' + p5)\n",
    "                                        #    else:\n",
    "                                        #        phonemes_with_combined_consonants.append(p1 + '+' + p2 + '+' + p3 + '+' + p4)\n",
    "                                        #        phonemes_with_combined_consonants.append(p5)\n",
    "                                        #    break                                                \n",
    "                                        #else:\n",
    "                                        #    phonemes_with_combined_consonants.append(p1 + '+' + p2 + '+' + p3 + '+' + p4)\n",
    "                                        #    break\n",
    "                                    else:\n",
    "                                        phonemes_with_combined_consonants.append(p1 + '+' + p2 + '+' + p3)\n",
    "                                        phonemes_with_combined_consonants.append(p4)\n",
    "                                    break\n",
    "                                else:\n",
    "                                    phonemes_with_combined_consonants.append(p1 + '+' + p2 + '+' + p3)\n",
    "                                    break\n",
    "                            else:\n",
    "                                phonemes_with_combined_consonants.append(p1 + '+' + p2)\n",
    "                                phonemes_with_combined_consonants.append(p3)\n",
    "                                break\n",
    "                        else:\n",
    "                            phonemes_with_combined_consonants.append(p1 + '+' + p2)\n",
    "                            break\n",
    "                    else:\n",
    "                        phonemes_with_combined_consonants.append(p1)\n",
    "                        phonemes_with_combined_consonants.append(p2)\n",
    "                        break\n",
    "                else:\n",
    "                    phonemes_with_combined_consonants.append(p1)\n",
    "                    break\n",
    "            else:\n",
    "                phonemes_with_combined_consonants.append(p1)\n",
    "                break\n",
    "    \n",
    "    return phonemes_with_combined_consonants\n",
    "\n",
    "\n",
    "def filter_dictionary_words(words, consonants, pronunciations, stresses, \n",
    "                            filter_words, filter_strings, filter_dictionary='english_words_py', \n",
    "                            verbose=False):\n",
    "    filtered_words = []\n",
    "    filtered_consonants = []\n",
    "    filtered_pronunciations = []\n",
    "    filtered_stresses = []\n",
    "    removed_words = []\n",
    "    for iword, word in enumerate(words): \n",
    "        if filter_dictionary == 'pyenchant':\n",
    "            if (enchant_dict.check(word) or enchant_dict.check(word.capitalize())) and \\\n",
    "                (word not in filter_words) and \\\n",
    "                (word not in filtered_words) and \\\n",
    "                all([x not in word for x in filter_strings]):\n",
    "                    filtered_words.append(word)\n",
    "                    filtered_consonants.append(consonants[iword])\n",
    "                    filtered_pronunciations.append(pronunciations[iword])\n",
    "                    filtered_stresses.append(stresses[iword])\n",
    "        elif filter_dictionary == 'english_words_py':\n",
    "            if (word in english_words_set or \\\n",
    "                    (word[:-1] in english_words_set and word[-1] == 's')) or \\\n",
    "                (word.capitalize() in english_words_set or \\\n",
    "                    (word.capitalize()[:-1] in english_words_set and word.capitalize()[-1] == 's')) and \\\n",
    "                (word not in filter_words) and \\\n",
    "                (word not in filtered_words) and \\\n",
    "                all([x not in word for x in filter_strings]):\n",
    "                    filtered_words.append(word)\n",
    "                    filtered_consonants.append(consonants[iword])\n",
    "                    filtered_pronunciations.append(pronunciations[iword])\n",
    "                    filtered_stresses.append(stresses[iword])\n",
    "        else:\n",
    "            removed_words.append(word)\n",
    "    \n",
    "    if verbose and removed_words != []:\n",
    "        print('{0} retained words, {1} removed words'.format(len(filtered_words),len(removed_words)))\n",
    "\n",
    "    return filtered_words, filtered_consonants, filtered_pronunciations, filtered_stresses, removed_words\n",
    "\n",
    "\n",
    "def save_object(obj, pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, \"wb\") as f:\n",
    "            pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as ex:\n",
    "        print(\"Error during pickling object:\", ex)\n",
    "\n",
    "        \n",
    "def load_object(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as ex:\n",
    "        print(\"Error during unpickling object:\", ex)\n",
    "\n",
    "\n",
    "def prepare_dictionary(filter_dictionary, filter_words_file, filter_strings, dictionary_folder):\n",
    "    '''\n",
    "    Filter CMU Pronunciation dictionary words and pronunciations.\n",
    "    Use a second dictionary of common words.\n",
    "    >>>index=10000\n",
    "    >>>print(all_words[index], all_consonants[index])\n",
    "    executive ['G+Z', 'K+Y', 'T', 'V'] \n",
    "    '''\n",
    "    cmu_entries = nltk.corpus.cmudict.entries()\n",
    "    cmu_words = []\n",
    "    cmu_consonants = []\n",
    "    cmu_pronunciations = []\n",
    "    cmu_pronunciations_stress = []\n",
    "    cmu_stresses = []\n",
    "    for cmu_word, cmu_pronunciation_stress in cmu_entries:\n",
    "        #print(cmu_word)\n",
    "        cmu_words.append(cmu_word.strip())\n",
    "        cmu_pronunciation_stress = combine_consonants(cmu_pronunciation_stress)\n",
    "        cmu_pronunciation_no_stress = [re.sub(r'\\d+', '', x) for x in cmu_pronunciation_stress]\n",
    "        cmu_pronunciations.append(cmu_pronunciation_no_stress)\n",
    "        cmu_consonants.append([x for x in cmu_pronunciation_no_stress if x in consonant_list])\n",
    "        cmu_stresses.append([re.sub(r'[A-Za-z\\+]', '', x) for x in cmu_pronunciation_stress])         \n",
    "    #for i in range(len(cmu_words)): \n",
    "    #    print(cmu_words[i], cmu_consonants[i], cmu_pronunciations[i], cmu_stresses[i])\n",
    "\n",
    "    print('Filter the CMU dictionary...')\n",
    "\n",
    "    # Load filter words\n",
    "    fread_filter = open(filter_words_file, \"r\")\n",
    "    filter_words = [x.strip() for x in fread_filter.readlines()]\n",
    "\n",
    "    all_words, all_consonants, all_pronunciations, all_stresses, nonwords = filter_dictionary_words(cmu_words, \n",
    "        cmu_consonants, cmu_pronunciations, cmu_stresses, \n",
    "        filter_words, filter_strings, filter_dictionary, verbose=False)    \n",
    "\n",
    "    save_object(all_words, dictionary_folder + 'words_{0}.pkl'.format(filter_dictionary))\n",
    "    save_object(all_consonants, dictionary_folder + 'consonants_{0}.pkl'.format(filter_dictionary))\n",
    "    save_object(all_pronunciations, dictionary_folder + 'pronunciations_{0}.pkl'.format(filter_dictionary))\n",
    "    save_object(all_stresses, dictionary_folder + 'stresses_{0}.pkl'.format(filter_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bc3bd",
   "metadata": {},
   "source": [
    "## Test different dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2608ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dictionaries = False\n",
    "if test_dictionaries:\n",
    "\n",
    "    import enchant\n",
    "    enchant_dict = enchant.Dict(\"en_US\")\n",
    "    #pip install cmudict\n",
    "    #nltk.download('cmudict')\n",
    "    #pip install pyenchant\n",
    "\n",
    "    # english-words-py (https://pypi.org/project/english-words/)\n",
    "    # \"Contains sets of English words from svnweb.freebsd.org/csrg/share/dict/. \n",
    "    # This is up to date with revision 61569 of their words list.\"\n",
    "    from english_words import english_words_set\n",
    "\n",
    "    # Most Common English Words (https://github.com/dolph/dictionary)\n",
    "    # \"enable1.txt (172,819), the more verbose version of the Official Scrabble Player's Dictionary \n",
    "    # (which is limited to words of 8 letters or less)\"\n",
    "    # \"popular.txt (25,322) represents the common subset of words found in both enable1.txt and Wiktionary's \n",
    "    # word frequency lists, which are in turn compiled by statistically analyzing a sample of 29 million \n",
    "    # words used in English TV and movie scripts.\"\n",
    "    enable1 = [line.rstrip() for line in open('data/dictionaries/enable1.txt')]\n",
    "    popular = [line.rstrip() for line in open('data/dictionaries/popular.txt')]\n",
    "\n",
    "    # NLTK words corpus:\n",
    "    #nltk.download('words')\n",
    "    from nltk.corpus import words\n",
    "    nltk_wordset = set(words.words())\n",
    "\n",
    "    # Wiktionary Word Frequency_lists (https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists#English)\n",
    "    #https://gist.github.com/h3xx/1976236\n",
    "    \n",
    "    print('nltk_wordset:      {0}'.format(len(nltk_wordset)))\n",
    "    print('enable1:           {0}'.format(len(enable1)))\n",
    "    print('pyenchant:         {0}'.format('?')) #len(enchant_dict.values())))\n",
    "    print('filtered CMU:      {0}'.format(len(all_words)))\n",
    "    print('english-words-py:  {0}'.format(len(english_words_set)))\n",
    "    print('popular:           {0}'.format(len(popular)))\n",
    "    print()\n",
    "    test_words = [\"can't\", 'geese', 'shelves', 'Thai', 'thai', 'e.', 'eure', 'bott', 'bitter', 'used']\n",
    "    for test_word in test_words:\n",
    "        print(test_word)\n",
    "        print('        NLTK words corpus:  {0}'.format(test_word in nltk_wordset))\n",
    "        print('        enable frequency:   {0}'.format(test_word in enable1))\n",
    "        print('        pyenchant spelling: {0}'.format(enchant_dict.check(test_word)))\n",
    "        print('        filtered CMU:       {0}'.format(test_word in all_words))\n",
    "        print('        english-words-py:   {0}'.format(test_word in english_words_set))\n",
    "        print('        popular frequency:  {0}'.format(test_word in popular))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349669b",
   "metadata": {},
   "source": [
    "    nltk_wordset:      235892\n",
    "    enable1:           172823\n",
    "    pyenchant:         ?\n",
    "    filtered CMU:      59539\n",
    "    english-words-py:  25487\n",
    "    popular:           25322\n",
    "\n",
    "    can't\n",
    "            NLTK words corpus:  False\n",
    "            enable frequency:   False\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   True\n",
    "            popular frequency:  False\n",
    "\n",
    "    geese\n",
    "            NLTK words corpus:  False\n",
    "            enable frequency:   True\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   True\n",
    "            popular frequency:  True\n",
    "\n",
    "    shelves\n",
    "            NLTK words corpus:  False\n",
    "            enable frequency:   True\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   False\n",
    "            popular frequency:  True\n",
    "\n",
    "    Thai\n",
    "            NLTK words corpus:  True\n",
    "            enable frequency:   False\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       False\n",
    "            english-words-py:   True\n",
    "            popular frequency:  False\n",
    "\n",
    "    thai\n",
    "            NLTK words corpus:  False\n",
    "            enable frequency:   False\n",
    "            pyenchant spelling: False\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   False\n",
    "            popular frequency:  False\n",
    "\n",
    "    e.\n",
    "            NLTK words corpus:  False\n",
    "            enable frequency:   False\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       False\n",
    "            english-words-py:   False\n",
    "            popular frequency:  False\n",
    "\n",
    "    eure\n",
    "            NLTK words corpus:  False\n",
    "            enable frequency:   False\n",
    "            pyenchant spelling: False\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   False\n",
    "            popular frequency:  False\n",
    "\n",
    "    bott\n",
    "            NLTK words corpus:  True\n",
    "            enable frequency:   True\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   False\n",
    "            popular frequency:  False\n",
    "\n",
    "    bitter\n",
    "            NLTK words corpus:  True\n",
    "            enable frequency:   True\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   False\n",
    "            popular frequency:  True\n",
    "\n",
    "    used\n",
    "            NLTK words corpus:  True\n",
    "            enable frequency:   True\n",
    "            pyenchant spelling: True\n",
    "            filtered CMU:       True\n",
    "            english-words-py:   False\n",
    "            popular frequency:  True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7cbdb",
   "metadata": {},
   "source": [
    "## Code to convert text to phonemes (and stresses and number of syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd3c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p  # pip install g2p_en\n",
    "word_to_phonemes = G2p()\n",
    "\n",
    "\n",
    "# Code to count syllables\n",
    "# https://datascience.stackexchange.com/questions/23376/how-to-get-the-number-of-syllables-in-a-word\n",
    "VOWEL_RUNS = re.compile(\"[aeiouy]+\", flags=re.I)\n",
    "EXCEPTIONS = re.compile(\n",
    "    # fixes trailing e issues:\n",
    "    # smite, scared\n",
    "    \"[^aeiou]e[sd]?$|\"\n",
    "    # fixes adverbs:\n",
    "    # nicely\n",
    "    + \"[^e]ely$\",\n",
    "    flags=re.I\n",
    ")\n",
    "ADDITIONAL = re.compile(\n",
    "    # fixes incorrect subtractions from exceptions:\n",
    "    # smile, scarred, raises, fated\n",
    "    \"[^aeioulr][lr]e[sd]?$|[csgz]es$|[td]ed$|\"\n",
    "    # fixes miscellaneous issues:\n",
    "    # flying, piano, video, prism, fire, evaluate\n",
    "    + \".y[aeiou]|ia(?!n$)|eo|ism$|[^aeiou]ire$|[^gq]ua\",\n",
    "    flags=re.I\n",
    ")\n",
    "def count_syllables(word):\n",
    "    vowel_runs = len(VOWEL_RUNS.findall(word))\n",
    "    exceptions = len(EXCEPTIONS.findall(word))\n",
    "    additional = len(ADDITIONAL.findall(word))\n",
    "    return max(1, vowel_runs - exceptions + additional)\n",
    "\n",
    "\n",
    "def words_to_sounds(words):\n",
    "    \n",
    "    phonemes = []\n",
    "    stresses = []\n",
    "    syllables = 0\n",
    "    for word in words:\n",
    "\n",
    "        # Extract phonemes per word (choose the first version of the phoneme)\n",
    "        #     :: multiple pronunciations: pronouncing.phones_for_word(word) \n",
    "        phonemes_and_stresses_for_word = word_to_phonemes(word)\n",
    "        phonemes_and_stresses_for_word = combine_consonants(phonemes_and_stresses_for_word)\n",
    "        phonemes_for_word = [re.sub(r'\\d+', '', x) for x in phonemes_and_stresses_for_word]\n",
    "        stresses_for_word = [re.sub(r\"(?:[A-Z])\",'', x) for x in phonemes_and_stresses_for_word]\n",
    "        phonemes_for_word = [x for x in phonemes_for_word if x in phoneme_list or x in consonant_list]                  \n",
    "        #print(word, phonemes_and_stresses_for_word, phonemes_for_word)\n",
    "\n",
    "        phonemes += phonemes_for_word  \n",
    "        stresses += stresses_for_word\n",
    "        syllables += count_syllables(word)\n",
    "\n",
    "    consonants = [x for x in phonemes if x in consonant_list] \n",
    "\n",
    "    return phonemes, consonants, stresses, syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1758a92",
   "metadata": {},
   "source": [
    "## Code to convert phonemes to candidate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b6962bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_numbers(numbers):\n",
    "    unique = []\n",
    "    for number in numbers:\n",
    "        if number not in unique:\n",
    "            unique.append(number)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def phonemes_to_candidate_words(phonemes, all_words, all_pronunciations, all_consonants, start=0,\n",
    "                                just_consonants=False): \n",
    "    '''\n",
    "    Generate a list of words from a list of phonemes,\n",
    "    by concatenating sequences of the phonemes \n",
    "    and searching in CMU's Pronunciation Dictionary.\n",
    "    \n",
    "    >>> phonemes_to_candidate_words(['HH', 'AY', 'D'], all_words, all_pronunciations, 0, False)\n",
    "    '''\n",
    "    words_from_phonemes = []\n",
    "    \n",
    "    # For each subsequence of phonemes\n",
    "    for stop in range(start + 1, len(phonemes) + 1):\n",
    "        \n",
    "        # Remove stresses from the subsequence of phonemes\n",
    "        phoneme_subset = [re.sub(r'\\d+', '', p) for p in phonemes[start:stop]]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(phoneme_subset)\n",
    "        \n",
    "        # Find words with matching consonants:\n",
    "        if just_consonants:\n",
    "            consonant_subset = [x for x in phoneme_subset if x in consonant_list]\n",
    "            if consonant_subset != []:\n",
    "                try:\n",
    "                    indices = [i for i,x in enumerate(all_consonants) if x == consonant_subset]\n",
    "                    for index in indices:\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        print(consonant_subset, all_consonants[index], all_words[index])\n",
    "                        words_from_phonemes.append([all_words[index], start, stop - 1])\n",
    "                except: pass\n",
    "                \n",
    "        # Find words with fully matching pronunciations:\n",
    "        else:\n",
    "            try:\n",
    "                indices = [i for i,x in enumerate(all_pronunciations) if x == phoneme_subset]\n",
    "                for index in indices:\n",
    "                    words_from_phonemes.append([all_words[index], start, stop - 1])\n",
    "            except: pass\n",
    "            \n",
    "    unique_stops = get_unique_numbers([i2 for x,i1,i2 in words_from_phonemes])\n",
    "\n",
    "    return words_from_phonemes, unique_stops\n",
    "\n",
    "\n",
    "# Code to find all words that sound like each segment of each phoneme list\n",
    "def phoneme_subsets_to_words(phonemes, all_words, all_pronunciations, all_consonants, \n",
    "                             just_consonants=False, ignore_words=None):\n",
    "\n",
    "    phoneme_words_original = []    \n",
    "    start = 0\n",
    "    unique_stops = [-1]\n",
    "    while start < len(phonemes):\n",
    "        if len(unique_stops) == 0:\n",
    "            unique_stops = [start + 1]\n",
    "        for stop in unique_stops:\n",
    "            start = stop + 1\n",
    "            if start < len(phonemes):\n",
    "                words_from_phonemes, unique_stops = phonemes_to_candidate_words(phonemes, \n",
    "                                                                                all_words, \n",
    "                                                                                all_pronunciations, \n",
    "                                                                                all_consonants, \n",
    "                                                                                start, \n",
    "                                                                                just_consonants)\n",
    "                phoneme_words_original += words_from_phonemes\n",
    "\n",
    "    if ignore_words:\n",
    "        phoneme_words = [] \n",
    "        for phoneme_word in phoneme_words_original:\n",
    "            if phoneme_word[0] not in ignore_words:\n",
    "                phoneme_words.append(phoneme_word)\n",
    "    else:\n",
    "        phoneme_words = phoneme_words_original\n",
    "    \n",
    "\n",
    "    return phoneme_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d01fb2",
   "metadata": {},
   "source": [
    "## Code to construct word sequences with matching phoneme stop and start indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313eab62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def copy_list(list_to_copy, ncopies):\n",
    "    list_copies = []\n",
    "    for i in range(ncopies):\n",
    "        list_copies.extend(list_to_copy)\n",
    "    return list_copies\n",
    "\n",
    "\n",
    "def flatten_list(nested_list):\n",
    "    '''\n",
    "    Flatten so that there are no tuples or lists within the list.\n",
    "    \n",
    "    >>> nested_list = [('e1d1', ('e1d2'), ['e2d1']), 'e3d0', [], ['e5d1']]\n",
    "    >>> flatten_list(nested_list)\n",
    "    ['e1d1', 'e1d2', 'e2d1', 'e3d0', 'e5d1']\n",
    "    '''\n",
    "    result=[]\n",
    "    if nested_list != []:\n",
    "        for element in nested_list:\n",
    "            if isinstance(element, list) or isinstance(element, tuple):\n",
    "                result.extend(flatten_list(element))\n",
    "            else:\n",
    "                result.append(element)\n",
    "    return result\n",
    "\n",
    "            \n",
    "def flatten_to_sublists_of_strings(nested_list):\n",
    "    '''\n",
    "    Flatten list to strings and sublists of strings.\n",
    "\n",
    "    >>> nested_list = [[[], '0', ('1',11,12), ('2',21,22), ['3',31,32]], [['4',41,42]]]\n",
    "    >>> flatten_to_sublists_of_strings(nested_list)\n",
    "    [[], '0', ['1', 11, 12], ['2', 21, 22], ['3', 31, 32], ['4', 41, 42]]\n",
    "    '''\n",
    "    result=[]\n",
    "    if nested_list == []:\n",
    "        result.extend([[]])\n",
    "    else:\n",
    "        if not any([isinstance(x, list)  for x in nested_list]) and \\\n",
    "           not any([isinstance(x, tuple) for x in nested_list]):\n",
    "            y=[]\n",
    "            for x in nested_list:\n",
    "                y.append(x)\n",
    "            result.append(y)\n",
    "        else:\n",
    "            for element in nested_list:\n",
    "                if isinstance(element, str):\n",
    "                    result.extend(element)\n",
    "                elif isinstance(element, list) or isinstance(element, tuple):\n",
    "                    if element == []:\n",
    "                        result.extend([[]])\n",
    "                    else:\n",
    "                        result.extend(flatten_to_sublists_of_strings(element))          \n",
    "    return result\n",
    "\n",
    "            \n",
    "def find_words_with_start_index(word_start_stop_list, start_index):\n",
    "    # store words that start at start_index\n",
    "    start_words = []\n",
    "    starts = []\n",
    "    stops = []\n",
    "    for word, start, stop in word_start_stop_list:\n",
    "        if start == start_index and start != []:\n",
    "            start_words.append(word)\n",
    "            starts.append(start)\n",
    "            stops.append(stop)\n",
    "            \n",
    "    return start_words, starts, stops\n",
    "\n",
    "\n",
    "def organize_words_by_start(words_list):\n",
    "\n",
    "    if not isinstance(words_list[0], list) and not isinstance(words_list[0], tuple):\n",
    "        words_list = [words_list]\n",
    "        \n",
    "    # Get unique starts and stops, and max start and stop\n",
    "    words2 = []\n",
    "    starts2 = []\n",
    "    stops2 = []\n",
    "    for word, start, stop in words_list:\n",
    "        words2.append(word)\n",
    "        starts2.append(start)\n",
    "        stops2.append(stop)\n",
    "    unique_starts = get_unique_numbers(starts2)\n",
    "    unique_stops = get_unique_numbers(stops2)\n",
    "    max_start = max(get_unique_numbers(starts2))\n",
    "    max_stop = max(get_unique_numbers(stops2))\n",
    "\n",
    "    # Words organized by start index\n",
    "    words_by_start = []\n",
    "    stops = []\n",
    "    for start_index in range(max_start + 1):\n",
    "        start_words, istarts, istops = find_words_with_start_index(words_list, start_index)\n",
    "        words_by_start.append(start_words)\n",
    "        stops.append(istops)        \n",
    "\n",
    "    return words_by_start, stops, unique_starts, unique_stops, max_start, max_stop\n",
    "\n",
    "\n",
    "def concatenate_lists(list_of_lists1, list_of_lists2):\n",
    "    result = []\n",
    "    for item1, item2 in zip(list_of_lists1, list_of_lists2):\n",
    "        if isinstance(item1, str) and isinstance(item2, list):\n",
    "            for element in item2:\n",
    "                result.append((item1, element))\n",
    "        elif isinstance(item1, list) and isinstance(item2, list):\n",
    "            result.append((item1 + item2))\n",
    "        elif isinstance(item1, tuple) and isinstance(item2, list):\n",
    "            result.append((list(item1) + item2))\n",
    "    return result\n",
    "\n",
    "\n",
    "def concatenate_words(prev_words, prev_stops, words_by_start, stops_by_start, unique_starts):\n",
    "    '''\n",
    "    Concatenate words where the stop index of one matches the start index of the next.\n",
    "    '''\n",
    "    # Initialize / format words\n",
    "    new_words = []\n",
    "    new_stops = []\n",
    "    words1 = prev_words\n",
    "    stops1 = prev_stops\n",
    " \n",
    "    # For each word that starts at a given index\n",
    "    for iword1, word1 in enumerate(words1):\n",
    "\n",
    "        # Find words that start after that word stops\n",
    "        word1_stop = stops1[iword1]\n",
    "        word2_start = word1_stop + 1\n",
    "        if word2_start in unique_starts:\n",
    "            words2 = words_by_start[word2_start]\n",
    "            stops2 = stops_by_start[word2_start]\n",
    "\n",
    "            # Concatenate the first word with each of the second set of words\n",
    "            if len(words2) > 0:\n",
    "                word1_copies = copy_list([word1], len(words2))\n",
    "                words2_list = [[x] for x in words2]\n",
    "                new_words.append(concatenate_lists(word1_copies, words2_list))\n",
    "                new_stops.append(stops2)\n",
    "\n",
    "    new_words = flatten_to_sublists_of_strings(new_words)\n",
    "    new_stops = flatten_list(new_stops)\n",
    "        \n",
    "    return new_words, new_stops\n",
    "\n",
    "\n",
    "def remove_duplicates(infile, outfile):\n",
    "    unique_lines = set(open(infile).readlines())\n",
    "    out = open(outfile, 'w').writelines(unique_lines)\n",
    "\n",
    "\n",
    "def words_stop_to_start(words_by_start, stops_by_start, unique_starts, max_stop, max_count, outfile=None):\n",
    "\n",
    "    # Initialize write to text file or to list\n",
    "    if outfile:\n",
    "        write_file = True\n",
    "        fwrite = open(outfile, \"w\")\n",
    "        fwrite.write('')\n",
    "        fwrite.close()\n",
    "        fwrite = open(outfile, \"a\")\n",
    "    else:\n",
    "        write_file = False\n",
    "    output_lines = []\n",
    "\n",
    "    # Initialize loop\n",
    "    prev_words = words_by_start[0]\n",
    "    prev_stops = stops_by_start[0]\n",
    "    count = 1\n",
    "    run = True\n",
    "    while(run):\n",
    "        count += 1\n",
    "\n",
    "        new_words, new_stops = concatenate_words(prev_words, prev_stops, \n",
    "                                                 words_by_start, stops_by_start, unique_starts)\n",
    "        \n",
    "        # Stop when all stops equal max_stop\n",
    "        if all([x == max_stop for x in new_stops]) or count == max_count:\n",
    "            run = False\n",
    "        \n",
    "        # Write to text file or to list\n",
    "        for istop, stop in enumerate(new_stops):\n",
    "            if stop == max_stop:\n",
    "                new_line = ' '.join(new_words[istop])\n",
    "                if write_file:\n",
    "                    fwrite.write(new_line + '\\n')\n",
    "                else:\n",
    "                    if new_line not in output_lines:            \n",
    "                        output_lines.append(new_line)\n",
    "\n",
    "        prev_words = new_words\n",
    "        prev_stops = new_stops\n",
    "\n",
    "    # After exiting the while loop, finalize\n",
    "    if write_file:\n",
    "        fwrite.close()\n",
    "        remove_duplicates(outfile, outfile)\n",
    "                                    \n",
    "    return output_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfeb3bd",
   "metadata": {},
   "source": [
    "## Homophone generator (same sounds, different words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee03382",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_homophones(phonemes, max_count, ignore_words=None, filename_base=None, \n",
    "                        verbose=True, verbose2=False):\n",
    "    \n",
    "    if filename_base:\n",
    "        filename = 'HOMOPHONES_' + filename_base + '.txt'\n",
    "\n",
    "    if verbose:\n",
    "        print('')\n",
    "        print('Input line:  {0}'.format(line.strip()), end='\\n')\n",
    "        print('Phonemes:    {0}'.format(', '.join(phonemes)), end='\\n')\n",
    "\n",
    "    phoneme_words = phoneme_subsets_to_words(phonemes, all_words, all_pronunciations, all_consonants, \n",
    "                                             just_consonants=False, ignore_words=ignore_words)\n",
    "\n",
    "    if verbose2:\n",
    "        print('Phoneme words:  {0}'.format(', '.join([x[0] for x in phoneme_words])), end='\\n')\n",
    "\n",
    "    if phoneme_words:\n",
    "        phoneme_words = flatten_to_sublists_of_strings(phoneme_words)\n",
    "        words_by_start, stops, unique_starts, x, y, max_stop = organize_words_by_start(phoneme_words)\n",
    "        \n",
    "        if verbose2:\n",
    "            words_by_start = flatten_to_sublists_of_strings(words_by_start)\n",
    "            print('Phoneme-generated words sorted by start index:  {0}'.format(words_by_start), end='\\n')\n",
    "\n",
    "        homophones = words_stop_to_start(words_by_start, stops, unique_starts, max_stop, max_count, filename)\n",
    "\n",
    "        if verbose and filename_base:\n",
    "            print('Phoneme-generated homophones written to {0}'.format(filename), end='\\n')\n",
    "        elif verbose and homophones != []:\n",
    "            print('Phoneme-generated homophones:', end='\\n')\n",
    "            for homophone in flatten_list(homophones):\n",
    "               print('    {0}'.format(homophone), end='\\n')\n",
    "    else:\n",
    "        homophones = None\n",
    "    \n",
    "    return homophones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ada1e",
   "metadata": {},
   "source": [
    "## Constonant generator (same consonants and consonant neighbors, different vowels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa1e9fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_constonants(phonemes, max_count, ignore_words=None, filename_base=None, \n",
    "                         verbose=True, verbose2=False):\n",
    "    \n",
    "    if filename_base:\n",
    "        filename = 'CONSTONANTS_' + filename_base + '.txt'\n",
    "\n",
    "    if verbose:\n",
    "        print('')\n",
    "        print('Input line:  {0}'.format(line.strip()), end='\\n')\n",
    "        print('Consonants:  {0}'.format(', '.join(consonants)), end='\\n')\n",
    "    \n",
    "    consonant_words = phoneme_subsets_to_words(phonemes, all_words, all_pronunciations, all_consonants, \n",
    "                                               just_consonants=True, ignore_words=ignore_words)\n",
    "\n",
    "    if verbose2:\n",
    "        print('Consonant words:  {0}'.format(', '.join([x[0] for x in consonant_words])), end='\\n')\n",
    "\n",
    "    if consonant_words:\n",
    "        consonant_words = flatten_to_sublists_of_strings(consonant_words)\n",
    "        words_by_start, stops, uniq_starts, x, y, max_stop = organize_words_by_start(consonant_words)\n",
    "\n",
    "        if verbose2:\n",
    "            words_by_start = flatten_to_sublists_of_strings(words_by_start)\n",
    "            print('Consonant-generated words sorted by start index:  {0}'.format(words_by_start), end='\\n')\n",
    "\n",
    "        constonants = words_stop_to_start(words_by_start, stops, uniq_starts, max_stop, max_count, filename)\n",
    "\n",
    "        if verbose and filename_base:\n",
    "            print('Consonant-generated constonants written to {0}'.format(filename), end='\\n')\n",
    "        elif verbose and constonants != []:\n",
    "            print('Consonant-generated constonants:', end='\\n')\n",
    "            for constonant in flatten_list(constonants):\n",
    "               print('    {0}'.format(constonant), end='\\n')\n",
    "    else:\n",
    "        constonants = None\n",
    "    \n",
    "    return constonants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecc794",
   "metadata": {},
   "source": [
    "## Prepare / load dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2929a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_prepare_dictionary = False\n",
    "\n",
    "dictionary_folder = \"data/dictionaries/\"\n",
    "filter_words_file = \"data/dictionaries/filter_words.txt\"\n",
    "filter_strings = ['.',',']\n",
    "filter_dictionary = 'english_words_py'  # 'pyenchant'\n",
    "\n",
    "if filter_dictionary == 'pyenchant':\n",
    "    import enchant\n",
    "    enchant_dict = enchant.Dict(\"en_US\")\n",
    "elif filter_dictionary == 'english_words_py':\n",
    "    from english_words import english_words_set\n",
    "\n",
    "if do_prepare_dictionary:\n",
    "    prepare_dictionary(filter_dictionary, filter_words_file, filter_strings, dictionary_folder)    \n",
    "else:\n",
    "    all_words = load_object(dictionary_folder + 'words_{0}.pkl'.format(filter_dictionary))\n",
    "    all_consonants = load_object(dictionary_folder + 'consonants_{0}.pkl'.format(filter_dictionary))\n",
    "    all_pronunciations = load_object(dictionary_folder + 'pronunciations_{0}.pkl'.format(filter_dictionary))\n",
    "    all_stresses = load_object(dictionary_folder + 'stresses_{0}.pkl'.format(filter_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424898e",
   "metadata": {},
   "source": [
    "## Run all code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6738eef3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input line:  The more things change...\n",
      "Consonants:  DH, M, R, TH, NG+Z, CH, N+JH\n",
      "['DH']\n",
      "['DH'] ['DH'] either\n",
      "['DH'] ['DH'] either\n",
      "['DH'] ['DH'] other\n",
      "['DH'] ['DH'] the\n",
      "['DH'] ['DH'] the\n",
      "['DH'] ['DH'] the\n",
      "['DH'] ['DH'] thee\n",
      "['DH'] ['DH'] they\n",
      "['DH'] ['DH'] thou\n",
      "['DH'] ['DH'] though\n",
      "['DH'] ['DH'] thy\n",
      "['DH', 'AH']\n",
      "['DH'] ['DH'] either\n",
      "['DH'] ['DH'] either\n",
      "['DH'] ['DH'] other\n",
      "['DH'] ['DH'] the\n",
      "['DH'] ['DH'] the\n",
      "['DH'] ['DH'] the\n",
      "['DH'] ['DH'] thee\n",
      "['DH'] ['DH'] they\n",
      "['DH'] ['DH'] thou\n",
      "['DH'] ['DH'] though\n",
      "['DH'] ['DH'] thy\n",
      "['DH', 'AH', 'M']\n",
      "['DH', 'M'] ['DH', 'M'] them\n",
      "['DH', 'M'] ['DH', 'M'] them\n",
      "['DH', 'AH', 'M', 'AO']\n",
      "['DH', 'M'] ['DH', 'M'] them\n",
      "['DH', 'M'] ['DH', 'M'] them\n",
      "['DH', 'AH', 'M', 'AO', 'R']\n",
      "['DH', 'AH', 'M', 'AO', 'R', 'TH']\n",
      "['DH', 'AH', 'M', 'AO', 'R', 'TH', 'IH']\n",
      "['DH', 'AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z']\n",
      "['DH', 'AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH']\n",
      "['DH', 'AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY']\n",
      "['DH', 'AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['AH']\n",
      "['AH', 'M']\n",
      "['M'] ['M'] aim\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] ami\n",
      "['M'] ['M'] ammo\n",
      "['M'] ['M'] amy\n",
      "['M'] ['M'] aroma\n",
      "['M'] ['M'] em\n",
      "['M'] ['M'] emery\n",
      "['M'] ['M'] emma\n",
      "['M'] ['M'] emory\n",
      "['M'] ['M'] him\n",
      "['M'] ['M'] i'm\n",
      "['M'] ['M'] irma\n",
      "['M'] ['M'] m\n",
      "['M'] ['M'] ma\n",
      "['M'] ['M'] mae\n",
      "['M'] ['M'] maier\n",
      "['M'] ['M'] mao\n",
      "['M'] ['M'] maria\n",
      "['M'] ['M'] marie\n",
      "['M'] ['M'] masts\n",
      "['M'] ['M'] maw\n",
      "['M'] ['M'] mawr\n",
      "['M'] ['M'] may\n",
      "['M'] ['M'] maya\n",
      "['M'] ['M'] mayer\n",
      "['M'] ['M'] mayo\n",
      "['M'] ['M'] mayor\n",
      "['M'] ['M'] me\n",
      "['M'] ['M'] meier\n",
      "['M'] ['M'] meow\n",
      "['M'] ['M'] meyer\n",
      "['M'] ['M'] mi\n",
      "['M'] ['M'] midday\n",
      "['M'] ['M'] mire\n",
      "['M'] ['M'] moe\n",
      "['M'] ['M'] moo\n",
      "['M'] ['M'] mow\n",
      "['M'] ['M'] moyer\n",
      "['M'] ['M'] mu\n",
      "['M'] ['M'] murray\n",
      "['M'] ['M'] my\n",
      "['M'] ['M'] myrrh\n",
      "['M'] ['M'] ohm\n",
      "['AH', 'M', 'AO']\n",
      "['M'] ['M'] aim\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] ami\n",
      "['M'] ['M'] ammo\n",
      "['M'] ['M'] amy\n",
      "['M'] ['M'] aroma\n",
      "['M'] ['M'] em\n",
      "['M'] ['M'] emery\n",
      "['M'] ['M'] emma\n",
      "['M'] ['M'] emory\n",
      "['M'] ['M'] him\n",
      "['M'] ['M'] i'm\n",
      "['M'] ['M'] irma\n",
      "['M'] ['M'] m\n",
      "['M'] ['M'] ma\n",
      "['M'] ['M'] mae\n",
      "['M'] ['M'] maier\n",
      "['M'] ['M'] mao\n",
      "['M'] ['M'] maria\n",
      "['M'] ['M'] marie\n",
      "['M'] ['M'] masts\n",
      "['M'] ['M'] maw\n",
      "['M'] ['M'] mawr\n",
      "['M'] ['M'] may\n",
      "['M'] ['M'] maya\n",
      "['M'] ['M'] mayer\n",
      "['M'] ['M'] mayo\n",
      "['M'] ['M'] mayor\n",
      "['M'] ['M'] me\n",
      "['M'] ['M'] meier\n",
      "['M'] ['M'] meow\n",
      "['M'] ['M'] meyer\n",
      "['M'] ['M'] mi\n",
      "['M'] ['M'] midday\n",
      "['M'] ['M'] mire\n",
      "['M'] ['M'] moe\n",
      "['M'] ['M'] moo\n",
      "['M'] ['M'] mow\n",
      "['M'] ['M'] moyer\n",
      "['M'] ['M'] mu\n",
      "['M'] ['M'] murray\n",
      "['M'] ['M'] my\n",
      "['M'] ['M'] myrrh\n",
      "['M'] ['M'] ohm\n",
      "['AH', 'M', 'AO', 'R']\n",
      "['M', 'R'] ['M', 'R'] maori\n",
      "['M', 'R'] ['M', 'R'] mar\n",
      "['M', 'R'] ['M', 'R'] mare\n",
      "['M', 'R'] ['M', 'R'] mario\n",
      "['M', 'R'] ['M', 'R'] marrow\n",
      "['M', 'R'] ['M', 'R'] marry\n",
      "['M', 'R'] ['M', 'R'] mary\n",
      "['M', 'R'] ['M', 'R'] mere\n",
      "['M', 'R'] ['M', 'R'] merry\n",
      "['M', 'R'] ['M', 'R'] mira\n",
      "['M', 'R'] ['M', 'R'] mire\n",
      "['M', 'R'] ['M', 'R'] mirror\n",
      "['M', 'R'] ['M', 'R'] mohr\n",
      "['M', 'R'] ['M', 'R'] moor\n",
      "['M', 'R'] ['M', 'R'] moore\n",
      "['M', 'R'] ['M', 'R'] more\n",
      "['M', 'R'] ['M', 'R'] morrow\n",
      "['M', 'R'] ['M', 'R'] morrow\n",
      "['M', 'R'] ['M', 'R'] myra\n",
      "['AH', 'M', 'AO', 'R', 'TH']\n",
      "['AH', 'M', 'AO', 'R', 'TH', 'IH']\n",
      "['AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z']\n",
      "['AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH']\n",
      "['AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY']\n",
      "['AH', 'M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['M']\n",
      "['M'] ['M'] aim\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] ami\n",
      "['M'] ['M'] ammo\n",
      "['M'] ['M'] amy\n",
      "['M'] ['M'] aroma\n",
      "['M'] ['M'] em\n",
      "['M'] ['M'] emery\n",
      "['M'] ['M'] emma\n",
      "['M'] ['M'] emory\n",
      "['M'] ['M'] him\n",
      "['M'] ['M'] i'm\n",
      "['M'] ['M'] irma\n",
      "['M'] ['M'] m\n",
      "['M'] ['M'] ma\n",
      "['M'] ['M'] mae\n",
      "['M'] ['M'] maier\n",
      "['M'] ['M'] mao\n",
      "['M'] ['M'] maria\n",
      "['M'] ['M'] marie\n",
      "['M'] ['M'] masts\n",
      "['M'] ['M'] maw\n",
      "['M'] ['M'] mawr\n",
      "['M'] ['M'] may\n",
      "['M'] ['M'] maya\n",
      "['M'] ['M'] mayer\n",
      "['M'] ['M'] mayo\n",
      "['M'] ['M'] mayor\n",
      "['M'] ['M'] me\n",
      "['M'] ['M'] meier\n",
      "['M'] ['M'] meow\n",
      "['M'] ['M'] meyer\n",
      "['M'] ['M'] mi\n",
      "['M'] ['M'] midday\n",
      "['M'] ['M'] mire\n",
      "['M'] ['M'] moe\n",
      "['M'] ['M'] moo\n",
      "['M'] ['M'] mow\n",
      "['M'] ['M'] moyer\n",
      "['M'] ['M'] mu\n",
      "['M'] ['M'] murray\n",
      "['M'] ['M'] my\n",
      "['M'] ['M'] myrrh\n",
      "['M'] ['M'] ohm\n",
      "['M', 'AO']\n",
      "['M'] ['M'] aim\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] am\n",
      "['M'] ['M'] ami\n",
      "['M'] ['M'] ammo\n",
      "['M'] ['M'] amy\n",
      "['M'] ['M'] aroma\n",
      "['M'] ['M'] em\n",
      "['M'] ['M'] emery\n",
      "['M'] ['M'] emma\n",
      "['M'] ['M'] emory\n",
      "['M'] ['M'] him\n",
      "['M'] ['M'] i'm\n",
      "['M'] ['M'] irma\n",
      "['M'] ['M'] m\n",
      "['M'] ['M'] ma\n",
      "['M'] ['M'] mae\n",
      "['M'] ['M'] maier\n",
      "['M'] ['M'] mao\n",
      "['M'] ['M'] maria\n",
      "['M'] ['M'] marie\n",
      "['M'] ['M'] masts\n",
      "['M'] ['M'] maw\n",
      "['M'] ['M'] mawr\n",
      "['M'] ['M'] may\n",
      "['M'] ['M'] maya\n",
      "['M'] ['M'] mayer\n",
      "['M'] ['M'] mayo\n",
      "['M'] ['M'] mayor\n",
      "['M'] ['M'] me\n",
      "['M'] ['M'] meier\n",
      "['M'] ['M'] meow\n",
      "['M'] ['M'] meyer\n",
      "['M'] ['M'] mi\n",
      "['M'] ['M'] midday\n",
      "['M'] ['M'] mire\n",
      "['M'] ['M'] moe\n",
      "['M'] ['M'] moo\n",
      "['M'] ['M'] mow\n",
      "['M'] ['M'] moyer\n",
      "['M'] ['M'] mu\n",
      "['M'] ['M'] murray\n",
      "['M'] ['M'] my\n",
      "['M'] ['M'] myrrh\n",
      "['M'] ['M'] ohm\n",
      "['M', 'AO', 'R']\n",
      "['M', 'R'] ['M', 'R'] maori\n",
      "['M', 'R'] ['M', 'R'] mar\n",
      "['M', 'R'] ['M', 'R'] mare\n",
      "['M', 'R'] ['M', 'R'] mario\n",
      "['M', 'R'] ['M', 'R'] marrow\n",
      "['M', 'R'] ['M', 'R'] marry\n",
      "['M', 'R'] ['M', 'R'] mary\n",
      "['M', 'R'] ['M', 'R'] mere\n",
      "['M', 'R'] ['M', 'R'] merry\n",
      "['M', 'R'] ['M', 'R'] mira\n",
      "['M', 'R'] ['M', 'R'] mire\n",
      "['M', 'R'] ['M', 'R'] mirror\n",
      "['M', 'R'] ['M', 'R'] mohr\n",
      "['M', 'R'] ['M', 'R'] moor\n",
      "['M', 'R'] ['M', 'R'] moore\n",
      "['M', 'R'] ['M', 'R'] more\n",
      "['M', 'R'] ['M', 'R'] morrow\n",
      "['M', 'R'] ['M', 'R'] morrow\n",
      "['M', 'R'] ['M', 'R'] myra\n",
      "['M', 'AO', 'R', 'TH']\n",
      "['M', 'AO', 'R', 'TH', 'IH']\n",
      "['M', 'AO', 'R', 'TH', 'IH', 'NG+Z']\n",
      "['M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH']\n",
      "['M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY']\n",
      "['M', 'AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['AO']\n",
      "['AO', 'R']\n",
      "['R'] ['R'] aerie\n",
      "['R'] ['R'] air\n",
      "['R'] ['R'] airy\n",
      "['R'] ['R'] are\n",
      "['R'] ['R'] area\n",
      "['R'] ['R'] aria\n",
      "['R'] ['R'] arrow\n",
      "['R'] ['R'] arrow\n",
      "['R'] ['R'] aura\n",
      "['R'] ['R'] aurora\n",
      "['R'] ['R'] ear\n",
      "['R'] ['R'] ear\n",
      "['R'] ['R'] eerie\n",
      "['R'] ['R'] era\n",
      "['R'] ['R'] era\n",
      "['R'] ['R'] ere\n",
      "['R'] ['R'] erie\n",
      "['R'] ['R'] err\n",
      "['R'] ['R'] error\n",
      "['R'] ['R'] heir\n",
      "['R'] ['R'] hour\n",
      "['R'] ['R'] ira\n",
      "['R'] ['R'] ire\n",
      "['R'] ['R'] oar\n",
      "['R'] ['R'] or\n",
      "['R'] ['R'] ore\n",
      "['R'] ['R'] orr\n",
      "['R'] ['R'] our\n",
      "['R'] ['R'] our\n",
      "['R'] ['R'] r\n",
      "['R'] ['R'] rae\n",
      "['R'] ['R'] raw\n",
      "['R'] ['R'] ray\n",
      "['R'] ['R'] re\n",
      "['R'] ['R'] re\n",
      "['R'] ['R'] rhea\n",
      "['R'] ['R'] rho\n",
      "['R'] ['R'] rio\n",
      "['R'] ['R'] roe\n",
      "['R'] ['R'] row\n",
      "['R'] ['R'] rowe\n",
      "['R'] ['R'] roy\n",
      "['R'] ['R'] rue\n",
      "['R'] ['R'] rusts\n",
      "['R'] ['R'] rye\n",
      "['R'] ['R'] wry\n",
      "['AO', 'R', 'TH']\n",
      "['R', 'TH'] ['R', 'TH'] roth\n",
      "['R', 'TH'] ['R', 'TH'] ruth\n",
      "['R', 'TH'] ['R', 'TH'] wrath\n",
      "['R', 'TH'] ['R', 'TH'] wreath\n",
      "['R', 'TH'] ['R', 'TH'] writhe\n",
      "['AO', 'R', 'TH', 'IH']\n",
      "['R', 'TH'] ['R', 'TH'] roth\n",
      "['R', 'TH'] ['R', 'TH'] ruth\n",
      "['R', 'TH'] ['R', 'TH'] wrath\n",
      "['R', 'TH'] ['R', 'TH'] wreath\n",
      "['R', 'TH'] ['R', 'TH'] writhe\n",
      "['AO', 'R', 'TH', 'IH', 'NG+Z']\n",
      "['AO', 'R', 'TH', 'IH', 'NG+Z', 'CH']\n",
      "['AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY']\n",
      "['AO', 'R', 'TH', 'IH', 'NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['R']\n",
      "['R'] ['R'] aerie\n",
      "['R'] ['R'] air\n",
      "['R'] ['R'] airy\n",
      "['R'] ['R'] are\n",
      "['R'] ['R'] area\n",
      "['R'] ['R'] aria\n",
      "['R'] ['R'] arrow\n",
      "['R'] ['R'] arrow\n",
      "['R'] ['R'] aura\n",
      "['R'] ['R'] aurora\n",
      "['R'] ['R'] ear\n",
      "['R'] ['R'] ear\n",
      "['R'] ['R'] eerie\n",
      "['R'] ['R'] era\n",
      "['R'] ['R'] era\n",
      "['R'] ['R'] ere\n",
      "['R'] ['R'] erie\n",
      "['R'] ['R'] err\n",
      "['R'] ['R'] error\n",
      "['R'] ['R'] heir\n",
      "['R'] ['R'] hour\n",
      "['R'] ['R'] ira\n",
      "['R'] ['R'] ire\n",
      "['R'] ['R'] oar\n",
      "['R'] ['R'] or\n",
      "['R'] ['R'] ore\n",
      "['R'] ['R'] orr\n",
      "['R'] ['R'] our\n",
      "['R'] ['R'] our\n",
      "['R'] ['R'] r\n",
      "['R'] ['R'] rae\n",
      "['R'] ['R'] raw\n",
      "['R'] ['R'] ray\n",
      "['R'] ['R'] re\n",
      "['R'] ['R'] re\n",
      "['R'] ['R'] rhea\n",
      "['R'] ['R'] rho\n",
      "['R'] ['R'] rio\n",
      "['R'] ['R'] roe\n",
      "['R'] ['R'] row\n",
      "['R'] ['R'] rowe\n",
      "['R'] ['R'] roy\n",
      "['R'] ['R'] rue\n",
      "['R'] ['R'] rusts\n",
      "['R'] ['R'] rye\n",
      "['R'] ['R'] wry\n",
      "['R', 'TH']\n",
      "['R', 'TH'] ['R', 'TH'] roth\n",
      "['R', 'TH'] ['R', 'TH'] ruth\n",
      "['R', 'TH'] ['R', 'TH'] wrath\n",
      "['R', 'TH'] ['R', 'TH'] wreath\n",
      "['R', 'TH'] ['R', 'TH'] writhe\n",
      "['R', 'TH', 'IH']\n",
      "['R', 'TH'] ['R', 'TH'] roth\n",
      "['R', 'TH'] ['R', 'TH'] ruth\n",
      "['R', 'TH'] ['R', 'TH'] wrath\n",
      "['R', 'TH'] ['R', 'TH'] wreath\n",
      "['R', 'TH'] ['R', 'TH'] writhe\n",
      "['R', 'TH', 'IH', 'NG+Z']\n",
      "['R', 'TH', 'IH', 'NG+Z', 'CH']\n",
      "['R', 'TH', 'IH', 'NG+Z', 'CH', 'EY']\n",
      "['R', 'TH', 'IH', 'NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['TH']\n",
      "['TH'] ['TH'] atheists\n",
      "['TH'] ['TH'] author\n",
      "['TH'] ['TH'] earth\n",
      "['TH'] ['TH'] earthy\n",
      "['TH'] ['TH'] eighth\n",
      "['TH'] ['TH'] ether\n",
      "['TH'] ['TH'] oath\n",
      "['TH'] ['TH'] thaw\n",
      "['TH'] ['TH'] thayer\n",
      "['TH'] ['TH'] thea\n",
      "['TH'] ['TH'] theorists\n",
      "['TH'] ['TH'] theory\n",
      "['TH'] ['TH'] thigh\n",
      "['TH'] ['TH'] thoreau\n",
      "['TH'] ['TH'] thorough\n",
      "['TH', 'IH']\n",
      "['TH'] ['TH'] atheists\n",
      "['TH'] ['TH'] author\n",
      "['TH'] ['TH'] earth\n",
      "['TH'] ['TH'] earthy\n",
      "['TH'] ['TH'] eighth\n",
      "['TH'] ['TH'] ether\n",
      "['TH'] ['TH'] oath\n",
      "['TH'] ['TH'] thaw\n",
      "['TH'] ['TH'] thayer\n",
      "['TH'] ['TH'] thea\n",
      "['TH'] ['TH'] theorists\n",
      "['TH'] ['TH'] theory\n",
      "['TH'] ['TH'] thigh\n",
      "['TH'] ['TH'] thoreau\n",
      "['TH'] ['TH'] thorough\n",
      "['TH', 'IH', 'NG+Z']\n",
      "['TH', 'NG+Z'] ['TH', 'NG+Z'] things\n",
      "['TH', 'IH', 'NG+Z', 'CH']\n",
      "['TH', 'IH', 'NG+Z', 'CH', 'EY']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TH', 'IH', 'NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['IH']\n",
      "['IH', 'NG+Z']\n",
      "['IH', 'NG+Z', 'CH']\n",
      "['IH', 'NG+Z', 'CH', 'EY']\n",
      "['IH', 'NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['NG+Z']\n",
      "['NG+Z', 'CH']\n",
      "['NG+Z', 'CH', 'EY']\n",
      "['NG+Z', 'CH', 'EY', 'N+JH']\n",
      "['EY']\n",
      "['EY', 'N+JH']\n",
      "['N+JH'] ['N+JH'] angie\n",
      "['N+JH'] ['N+JH'] arrange\n",
      "['N+JH'] ['N+JH'] injure\n",
      "['N+JH'] ['N+JH'] injury\n",
      "Consonant words:  either, either, other, the, the, the, thee, they, thou, though, thy, either, either, other, the, the, the, thee, they, thou, though, thy, them, them, them, them, aim, am, am, ami, ammo, amy, aroma, em, emery, emma, emory, him, i'm, irma, m, ma, mae, maier, mao, maria, marie, masts, maw, mawr, may, maya, mayer, mayo, mayor, me, meier, meow, meyer, mi, midday, mire, moe, moo, mow, moyer, mu, murray, my, myrrh, ohm, aim, am, am, ami, ammo, amy, aroma, em, emery, emma, emory, him, i'm, irma, m, ma, mae, maier, mao, maria, marie, masts, maw, mawr, may, maya, mayer, mayo, mayor, me, meier, meow, meyer, mi, midday, mire, moe, moo, mow, moyer, mu, murray, my, myrrh, ohm, maori, mar, mare, mario, marrow, marry, mary, mere, merry, mira, mire, mirror, mohr, moor, moore, more, morrow, morrow, myra, aim, am, am, ami, ammo, amy, aroma, em, emery, emma, emory, him, i'm, irma, m, ma, mae, maier, mao, maria, marie, masts, maw, mawr, may, maya, mayer, mayo, mayor, me, meier, meow, meyer, mi, midday, mire, moe, moo, mow, moyer, mu, murray, my, myrrh, ohm, aim, am, am, ami, ammo, amy, aroma, em, emery, emma, emory, him, i'm, irma, m, ma, mae, maier, mao, maria, marie, masts, maw, mawr, may, maya, mayer, mayo, mayor, me, meier, meow, meyer, mi, midday, mire, moe, moo, mow, moyer, mu, murray, my, myrrh, ohm, maori, mar, mare, mario, marrow, marry, mary, mere, merry, mira, mire, mirror, mohr, moor, moore, more, morrow, morrow, myra, aerie, air, airy, are, area, aria, arrow, arrow, aura, aurora, ear, ear, eerie, era, era, ere, erie, err, error, heir, hour, ira, ire, oar, or, ore, orr, our, our, r, rae, raw, ray, re, re, rhea, rho, rio, roe, row, rowe, roy, rue, rusts, rye, wry, roth, ruth, wrath, wreath, writhe, roth, ruth, wrath, wreath, writhe, aerie, air, airy, are, area, aria, arrow, arrow, aura, aurora, ear, ear, eerie, era, era, ere, erie, err, error, heir, hour, ira, ire, oar, or, ore, orr, our, our, r, rae, raw, ray, re, re, rhea, rho, rio, roe, row, rowe, roy, rue, rusts, rye, wry, roth, ruth, wrath, wreath, writhe, roth, ruth, wrath, wreath, writhe, atheists, author, earth, earthy, eighth, ether, oath, thaw, thayer, thea, theorists, theory, thigh, thoreau, thorough, atheists, author, earth, earthy, eighth, ether, oath, thaw, thayer, thea, theorists, theory, thigh, thoreau, thorough, things, angie, arrange, injure, injury\n",
      "Consonant-generated words sorted by start index:  [['either', 'either', 'other', 'the', 'the', 'the', 'thee', 'they', 'thou', 'though', 'thy', 'either', 'either', 'other', 'the', 'the', 'the', 'thee', 'they', 'thou', 'though', 'thy', 'them', 'them', 'them', 'them'], ['aim', 'am', 'am', 'ami', 'ammo', 'amy', 'aroma', 'em', 'emery', 'emma', 'emory', 'him', \"i'm\", 'irma', 'm', 'ma', 'mae', 'maier', 'mao', 'maria', 'marie', 'masts', 'maw', 'mawr', 'may', 'maya', 'mayer', 'mayo', 'mayor', 'me', 'meier', 'meow', 'meyer', 'mi', 'midday', 'mire', 'moe', 'moo', 'mow', 'moyer', 'mu', 'murray', 'my', 'myrrh', 'ohm', 'aim', 'am', 'am', 'ami', 'ammo', 'amy', 'aroma', 'em', 'emery', 'emma', 'emory', 'him', \"i'm\", 'irma', 'm', 'ma', 'mae', 'maier', 'mao', 'maria', 'marie', 'masts', 'maw', 'mawr', 'may', 'maya', 'mayer', 'mayo', 'mayor', 'me', 'meier', 'meow', 'meyer', 'mi', 'midday', 'mire', 'moe', 'moo', 'mow', 'moyer', 'mu', 'murray', 'my', 'myrrh', 'ohm', 'maori', 'mar', 'mare', 'mario', 'marrow', 'marry', 'mary', 'mere', 'merry', 'mira', 'mire', 'mirror', 'mohr', 'moor', 'moore', 'more', 'morrow', 'morrow', 'myra'], ['aim', 'am', 'am', 'ami', 'ammo', 'amy', 'aroma', 'em', 'emery', 'emma', 'emory', 'him', \"i'm\", 'irma', 'm', 'ma', 'mae', 'maier', 'mao', 'maria', 'marie', 'masts', 'maw', 'mawr', 'may', 'maya', 'mayer', 'mayo', 'mayor', 'me', 'meier', 'meow', 'meyer', 'mi', 'midday', 'mire', 'moe', 'moo', 'mow', 'moyer', 'mu', 'murray', 'my', 'myrrh', 'ohm', 'aim', 'am', 'am', 'ami', 'ammo', 'amy', 'aroma', 'em', 'emery', 'emma', 'emory', 'him', \"i'm\", 'irma', 'm', 'ma', 'mae', 'maier', 'mao', 'maria', 'marie', 'masts', 'maw', 'mawr', 'may', 'maya', 'mayer', 'mayo', 'mayor', 'me', 'meier', 'meow', 'meyer', 'mi', 'midday', 'mire', 'moe', 'moo', 'mow', 'moyer', 'mu', 'murray', 'my', 'myrrh', 'ohm', 'maori', 'mar', 'mare', 'mario', 'marrow', 'marry', 'mary', 'mere', 'merry', 'mira', 'mire', 'mirror', 'mohr', 'moor', 'moore', 'more', 'morrow', 'morrow', 'myra'], ['aerie', 'air', 'airy', 'are', 'area', 'aria', 'arrow', 'arrow', 'aura', 'aurora', 'ear', 'ear', 'eerie', 'era', 'era', 'ere', 'erie', 'err', 'error', 'heir', 'hour', 'ira', 'ire', 'oar', 'or', 'ore', 'orr', 'our', 'our', 'r', 'rae', 'raw', 'ray', 're', 're', 'rhea', 'rho', 'rio', 'roe', 'row', 'rowe', 'roy', 'rue', 'rusts', 'rye', 'wry', 'roth', 'ruth', 'wrath', 'wreath', 'writhe', 'roth', 'ruth', 'wrath', 'wreath', 'writhe'], ['aerie', 'air', 'airy', 'are', 'area', 'aria', 'arrow', 'arrow', 'aura', 'aurora', 'ear', 'ear', 'eerie', 'era', 'era', 'ere', 'erie', 'err', 'error', 'heir', 'hour', 'ira', 'ire', 'oar', 'or', 'ore', 'orr', 'our', 'our', 'r', 'rae', 'raw', 'ray', 're', 're', 'rhea', 'rho', 'rio', 'roe', 'row', 'rowe', 'roy', 'rue', 'rusts', 'rye', 'wry', 'roth', 'ruth', 'wrath', 'wreath', 'writhe', 'roth', 'ruth', 'wrath', 'wreath', 'writhe'], ['atheists', 'author', 'earth', 'earthy', 'eighth', 'ether', 'oath', 'thaw', 'thayer', 'thea', 'theorists', 'theory', 'thigh', 'thoreau', 'thorough', 'atheists', 'author', 'earth', 'earthy', 'eighth', 'ether', 'oath', 'thaw', 'thayer', 'thea', 'theorists', 'theory', 'thigh', 'thoreau', 'thorough', 'things'], [], [], [], ['angie', 'arrange', 'injure', 'injury']]\n",
      "Consonant-generated constonants written to CONSTONANTS_The_more_things_change....txt\n"
     ]
    }
   ],
   "source": [
    "# Choose morphing operations:\n",
    "do_generate_homophones = 0#True\n",
    "do_ignore_inputs1 = False\n",
    "\n",
    "do_generate_constonants = True\n",
    "do_ignore_inputs2 = False\n",
    "\n",
    "\n",
    "#homophonify_anagram = True\n",
    "#homophonify_loop = True\n",
    "#alliterate = True\n",
    "#swap_sounds = True\n",
    "#beat = True\n",
    "#translate = True\n",
    "#rhyme = False\n",
    "\n",
    "verbose = True\n",
    "verbose2 = True #False\n",
    "\n",
    "# Load text\n",
    "fread = open(\"demo.txt\", \"r\")\n",
    "lines = fread.readlines()\n",
    "for line in lines:        \n",
    "    words = line.split()\n",
    "    words_lower = [re.sub(r'[^\\'A-Za-z]', '', x).lower() for x in words]\n",
    "    nwords = len(''.join(words))\n",
    "    filename_base = '_'.join(words)\n",
    "    if do_ignore_inputs1:\n",
    "        ignore_inputs1 = words_lower\n",
    "    else:\n",
    "        ignore_inputs1 = []\n",
    "    if do_ignore_inputs2:\n",
    "        ignore_inputs2 = words_lower\n",
    "    else:\n",
    "        ignore_inputs2 = []\n",
    "        \n",
    "    phonemes, consonants, stresses, syllables = words_to_sounds(words_lower)\n",
    "    \n",
    "    if do_generate_homophones:\n",
    "        homophones = generate_homophones(phonemes, nwords, ignore_inputs1, filename_base, verbose, verbose2)\n",
    "\n",
    "    if do_generate_constonants:\n",
    "        constonants = generate_constonants(phonemes, nwords, ignore_inputs2, filename_base, verbose, verbose2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1863d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
