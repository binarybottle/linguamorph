{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc51f1a6",
   "metadata": {},
   "source": [
    "## Phonemes and Carnegie Mellon Pronouncing Dictionary\n",
    "\n",
    "(See https://github.com/cmusphinx/cmudict/tree/4c6a365cea2c34340ffc218d5af7a38920fa7e37)\n",
    "\n",
    "From https://www.nltk.org/_modules/nltk/corpus/reader/cmudict.html:\n",
    "\n",
    "The Carnegie Mellon Pronouncing Dictionary [cmudict.0.6]\n",
    "Copyright 1998 Carnegie Mellon University\n",
    "\n",
    "File Format: Each line consists of an uppercased word, a counter\n",
    "(for alternative pronunciations), and a transcription.  Vowels are\n",
    "marked for stress (1=primary, 2=secondary, 0=no stress).  E.g.:\n",
    "NATURAL 1 N AE1 CH ER0 AH0 L\n",
    "\n",
    "The dictionary contains 127069 entries.  Of these, 119400 words are assigned\n",
    "a unique pronunciation, 6830 words have two pronunciations, and 839 words have\n",
    "three or more pronunciations.  Many of these are fast-speech variants.\n",
    "\n",
    "Phonemes: There are 39 phonemes, as shown below:\n",
    "\n",
    "    Phoneme Example Translation    Phoneme Example Translation\n",
    "    ------- ------- -----------    ------- ------- -----------\n",
    "    AA      odd     AA D           AE      at      AE T\n",
    "    AH      hut     HH AH T        AO      ought   AO T\n",
    "    AW      cow     K AW           AY      hide    HH AY D\n",
    "    B       be      B IY           CH      cheese  CH IY Z\n",
    "    D       dee     D IY           DH      thee    DH IY\n",
    "    EH      Ed      EH D           ER      hurt    HH ER T\n",
    "    EY      ate     EY T           F       fee     F IY\n",
    "    G       green   G R IY N       HH      he      HH IY\n",
    "    IH      it      IH T           IY      eat     IY T\n",
    "    JH      gee     JH IY          K       key     K IY\n",
    "    L       lee     L IY           M       me      M IY\n",
    "    N       knee    N IY           NG      ping    P IH NG\n",
    "    OW      oat     OW T           OY      toy     T OY\n",
    "    P       pee     P IY           R       read    R IY D\n",
    "    S       sea     S IY           SH      she     SH IY\n",
    "    T       tea     T IY           TH      theta   TH EY T AH\n",
    "    UH      hood    HH UH D        UW      two     T UW\n",
    "    V       vee     V IY           W       we      W IY\n",
    "    Y       yield   Y IY L D       Z       zee     Z IY\n",
    "    ZH      seizure S IY ZH ER\n",
    "    \n",
    "From https://www.pythonstudio.us/language-processing/a-pronouncing-dictionary.html:\n",
    "\n",
    "For each word, this lexicon provides a list of phonetic codes—distinct labels for each contrastive sound—known as phones. Observe that fire has two pronunciations (in U.S. English): the one-syllable F AY1 R, and the two-syllable F AY1 ER0. The symbols in the CMU Pronouncing Dictionary are from the Arpabet, described in more detail at http://en.wikipedia.org/wiki/Arpabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2608ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import enchant\n",
    "\n",
    "entries = nltk.corpus.cmudict.entries()\n",
    "my_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "phoneme_list = ['AA','AH','AW','B','D','EH','EY','G','IH','JH','L','N','OW','P','S','T','UH','V','Y','ZH',\n",
    "                'AE','AO','AY','CH','DH','ER','F','HH','IY','K','M','NG','OY','R','SH','TH','UW','W','Z']\n",
    "phoneme_vowel_list = ['AA','AH','AW','EH','EY','IH','OW','UH','AE','AO','AY','ER','IY','OY','UW']\n",
    "phoneme_consonant_list = ['B','D','G','JH','L','N','P','S','T','V','Y','ZH','CH',\n",
    "                          'DH','F','HH','K','M','NG','R','SH','TH','W','Z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1758a92",
   "metadata": {},
   "source": [
    "## Code to convert words to phonemes and phonemes to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b6962bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "word_to_phonemes = G2p()\n",
    "\n",
    "\n",
    "def get_unique_numbers(numbers):\n",
    "    unique = []\n",
    "    for number in numbers:\n",
    "        if number not in unique:\n",
    "            unique.append(number)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def phonemes_to_candidate_words(phonemes, start=0): \n",
    "    '''\n",
    "    Generate a list of words from a list of phonemes,\n",
    "    by concatenating sequences of the phonemes \n",
    "    and searching in CMU's Pronunciation Dictionary.\n",
    "    '''\n",
    "    words_from_phonemes = []\n",
    "    words_from_consonants = []\n",
    "    for stop in range(start + 1, len(phonemes) + 1):\n",
    "        phoneme_subset = phonemes[start:stop]\n",
    "        for word, pron in entries:\n",
    "            \n",
    "            # Find matching phonemes (homonym)\n",
    "            if len(pron) == len(phoneme_subset):\n",
    "                match = 0\n",
    "                for index, p in enumerate(pron):\n",
    "                    if re.sub(r'\\d+', '', p) == re.sub(r'\\d+', '', phoneme_subset[index]):\n",
    "                        match += 1\n",
    "                if match == len(pron):\n",
    "                    words_from_phonemes.append([word, start, start + index])\n",
    "\n",
    "            # Find matching consonants\n",
    "            if len(pron) >= len(phoneme_subset):\n",
    "                \n",
    "                pron_consonant_subset = [x for x in pron if x in phoneme_consonant_list]\n",
    "                if pron_consonant_subset != []:\n",
    "                    consonant_subset = [x for x in phoneme_subset if x in phoneme_consonant_list]\n",
    "                    if consonant_subset != []:\n",
    "                        if len(pron_consonant_subset) == len(consonant_subset):\n",
    "                            match = 0\n",
    "                            for index, p in enumerate(pron_consonant_subset):\n",
    "                                if p == consonant_subset[index]:\n",
    "                                    match += 1\n",
    "                            if match == len(consonant_subset):\n",
    "                                words_from_consonants.append([word, start, start + index])\n",
    "                    \n",
    "    unique_stops = get_unique_numbers([i2 for x,i1,i2 in words_from_phonemes])\n",
    "    unique_stops_consonants = get_unique_numbers([i2 for x,i1,i2 in words_from_consonants])\n",
    "\n",
    "    return words_from_phonemes, unique_stops, words_from_consonants, unique_stops_consonants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b563a4",
   "metadata": {},
   "source": [
    "## Code to count syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c01e57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/23376/how-to-get-the-number-of-syllables-in-a-word\n",
    "\n",
    "import re\n",
    "\n",
    "VOWEL_RUNS = re.compile(\"[aeiouy]+\", flags=re.I)\n",
    "EXCEPTIONS = re.compile(\n",
    "    # fixes trailing e issues:\n",
    "    # smite, scared\n",
    "    \"[^aeiou]e[sd]?$|\"\n",
    "    # fixes adverbs:\n",
    "    # nicely\n",
    "    + \"[^e]ely$\",\n",
    "    flags=re.I\n",
    ")\n",
    "ADDITIONAL = re.compile(\n",
    "    # fixes incorrect subtractions from exceptions:\n",
    "    # smile, scarred, raises, fated\n",
    "    \"[^aeioulr][lr]e[sd]?$|[csgz]es$|[td]ed$|\"\n",
    "    # fixes miscellaneous issues:\n",
    "    # flying, piano, video, prism, fire, evaluate\n",
    "    + \".y[aeiou]|ia(?!n$)|eo|ism$|[^aeiou]ire$|[^gq]ua\",\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "def count_syllables(word):\n",
    "    vowel_runs = len(VOWEL_RUNS.findall(word))\n",
    "    exceptions = len(EXCEPTIONS.findall(word))\n",
    "    additional = len(ADDITIONAL.findall(word))\n",
    "    return max(1, vowel_runs - exceptions + additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7cbdb",
   "metadata": {},
   "source": [
    "## Code to extract phonemes, stresses, and number of syllables per line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cd3c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_sounds(line):\n",
    "\n",
    "    if line.strip() != \"\":\n",
    "        words = line.split()\n",
    "        phonemes = []\n",
    "        stresses = []\n",
    "        syllables = 0\n",
    "        for word in words:\n",
    "\n",
    "            # Extract phonemes per word (choose the first version of the phoneme)\n",
    "            #     :: multiple pronunciations: pronouncing.phones_for_word(word) \n",
    "            phonemes_and_stresses_for_word = word_to_phonemes(word)\n",
    "                      \n",
    "            phonemes_for_word = [re.sub(r'\\d+', '', x) for x in phonemes_and_stresses_for_word]\n",
    "            stresses_blanks_for_word = [re.sub(r\"(?:[A-Z])\",'', x) for x in phonemes_and_stresses_for_word]\n",
    "            stresses_for_word = []\n",
    "            for i,p in enumerate(phonemes_for_word):\n",
    "                if p in phoneme_list:\n",
    "                    if stresses_blanks_for_word[i] == '':\n",
    "                        stresses_for_word.append(0)\n",
    "                    elif stresses_blanks_for_word[i] == '0':\n",
    "                        stresses_for_word.append(0)\n",
    "                    elif stresses_blanks_for_word[i] == '1':\n",
    "                        stresses_for_word.append(1)\n",
    "                    elif stresses_blanks_for_word[i] == '2':\n",
    "                        stresses_for_word.append(2)                        \n",
    "            phonemes_for_word = [x for x in phonemes_for_word if x in phoneme_list]                  \n",
    "            phonemes += phonemes_for_word  \n",
    "            stresses += stresses_for_word\n",
    "            syllables += count_syllables(word)\n",
    "\n",
    "        consonants = [x for x in phonemes if x in phoneme_consonant_list] \n",
    "\n",
    "    return phonemes, consonants, stresses, syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39200be",
   "metadata": {},
   "source": [
    "## Find all words that sound like each segment of each phoneme list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "100a2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phoneme_subsets_to_words(phonemes):\n",
    "\n",
    "    phoneme_words = []\n",
    "    consonant_words = []\n",
    "    start = 0\n",
    "    unique_stops = [-1]\n",
    "    while start < len(phonemes):\n",
    "        if len(unique_stops) == 0:\n",
    "            unique_stops = [start + 1]\n",
    "        for stop in unique_stops:\n",
    "            start = stop + 1\n",
    "            if start < len(phonemes):\n",
    "                words_from_phonemes, unique_stops, words_from_consonants, unique_stops_consonants = phonemes_to_candidate_words(phonemes, start)\n",
    "                phoneme_words += words_from_phonemes\n",
    "                consonant_words += words_from_consonants\n",
    "\n",
    "    return phoneme_words, consonant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07b7ea",
   "metadata": {},
   "source": [
    "## Filter words by another English dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa2f3a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(my_dict.check(\"Thai\"))\n",
    "\n",
    "def filter_dictionary_words(words, verbose=False):\n",
    "\n",
    "    filtered_words = []\n",
    "    removed_words = []\n",
    "    for word in words: \n",
    "        if my_dict.check(word[0]):\n",
    "            filtered_words.append(word)\n",
    "        else:\n",
    "            removed_words.append(word)\n",
    "    \n",
    "    if verbose and removed_words != []:\n",
    "        print('Removed words:  {0}'.format(', '.join([x[0] for x in removed_words])))\n",
    "\n",
    "    return filtered_words, removed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d01fb2",
   "metadata": {},
   "source": [
    "## Organize words by their phoneme start and stop indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "313eab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_list(list_to_copy, ncopies):\n",
    "    list_copies = []\n",
    "    for i in range(ncopies):\n",
    "        list_copies.extend(list_to_copy)\n",
    "    return list_copies\n",
    "\n",
    "\n",
    "def flatten_list(nested_list):\n",
    "    '''\n",
    "    Flatten so that there are no tuples or lists within the list.\n",
    "    \n",
    "    >>> nested_list = [(['tye', 'a'], 'ja')]\n",
    "    >>> flatten_list(nested_list)\n",
    "    ... ['tye', 'a', 'ja']\n",
    "    '''\n",
    "    result=[]\n",
    "    if nested_list != []:\n",
    "        for element in nested_list:\n",
    "            if isinstance(element, list) or isinstance(element, tuple):\n",
    "                result.extend(flatten_list(element))\n",
    "            else:\n",
    "                result.append(element)\n",
    "    return result\n",
    "\n",
    "            \n",
    "def flatten_sublists(nested_list):\n",
    "    '''\n",
    "    Flatten so that there are no subsublists within the sublists.\n",
    "    \n",
    "    >>> nested_list = [[('pty', 'a'), ('pty', 'uh'), ('pty', 'uhh')], [('tae', 'a'), ('tae', 'uh'), ('tae', 'uhh')]]\n",
    "    >>> flatten_sublists(nested_list)\n",
    "    [('pty', 'a'),\n",
    "     ('pty', 'uh'),\n",
    "     ('pty', 'uhh'),\n",
    "     ('tae', 'a'),\n",
    "     ('tae', 'uh'),\n",
    "     ('tae', 'uhh')]\n",
    "    '''\n",
    "    result=[]\n",
    "    if nested_list != []:\n",
    "        for element in nested_list:\n",
    "            if isinstance(element, list) or isinstance(element, tuple):\n",
    "                if element != []:\n",
    "                    if isinstance(element[0], list) or isinstance(element[0], tuple):\n",
    "                        result.extend(flatten_sublists(element))\n",
    "                    else:\n",
    "                        result.append(element)\n",
    "    return result\n",
    "\n",
    "            \n",
    "def find_words_with_start_index(word_start_stop_list, start_index):\n",
    "    # store words that start at start_index\n",
    "    start_words = []\n",
    "    starts = []\n",
    "    stops = []\n",
    "    for word, start, stop in word_start_stop_list:\n",
    "        if start == start_index and start != []:\n",
    "            start_words.append(word)\n",
    "            starts.append(start)\n",
    "            stops.append(stop)\n",
    "            \n",
    "    return start_words, starts, stops\n",
    "\n",
    "\n",
    "def organize_words_by_start(words_list):\n",
    "\n",
    "    if not isinstance(words_list[0], list) and not isinstance(words_list[0], tuple):\n",
    "        words_list = [words_list]\n",
    "        \n",
    "    # Get unique starts and stops, and max start and stop\n",
    "    words2 = []\n",
    "    starts2 = []\n",
    "    stops2 = []\n",
    "    for word, start, stop in words_list:\n",
    "        words2.append(word)\n",
    "        starts2.append(start)\n",
    "        stops2.append(stop)\n",
    "    unique_starts = get_unique_numbers(starts2)\n",
    "    unique_stops = get_unique_numbers(stops2)\n",
    "    max_start = max(get_unique_numbers(starts2))\n",
    "    max_stop = max(get_unique_numbers(stops2))\n",
    "\n",
    "    # Words organized by start index\n",
    "    words_by_start = []\n",
    "    stops = []\n",
    "    for start_index in range(max_start + 1):\n",
    "        start_words, istarts, istops = find_words_with_start_index(words_list, start_index)\n",
    "        words_by_start.append(start_words)\n",
    "        stops.append(istops)        \n",
    "    #stops = stops[0:-1] \n",
    "\n",
    "    return words_by_start, stops, unique_starts, unique_stops, max_start, max_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843331d",
   "metadata": {},
   "source": [
    "## Construct word sequences with matching phoneme stop and start indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b57a5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_lists(list_of_lists1, list_of_lists2):\n",
    "    result = []\n",
    "    for item1, item2 in zip(list_of_lists1, list_of_lists2):\n",
    "        if isinstance(item1, str) and isinstance(item2, list):\n",
    "            for element in item2:\n",
    "                result.append((item1, element))\n",
    "        elif isinstance(item1, tuple) and isinstance(item2, list):\n",
    "            result.append((list(item1) + list(item2)))\n",
    "    return result\n",
    "\n",
    "\n",
    "def concatenate_words(new_words, new_stops, words, stops, unique_starts):\n",
    "    '''\n",
    "    Concatenate words where the stop index of one matches the start index of the next.\n",
    "    '''\n",
    "    # Initialize / format words\n",
    "    if new_words == []:\n",
    "        words1 = words[0]\n",
    "        stops1 = stops[0]\n",
    "    else:\n",
    "        words1 = flatten_sublists(new_words)\n",
    "        stops1 = flatten_list(new_stops)\n",
    "\n",
    "    # For each word that starts at  a given index\n",
    "    for iword1, word1 in enumerate(words1):\n",
    "\n",
    "        # Find words that start after that word stops\n",
    "        if isinstance(stops1, int):\n",
    "            stops1 = [stops1]\n",
    "\n",
    "        word1_stop = stops1[iword1]\n",
    "        word2_start = word1_stop + 1\n",
    "        if word2_start in unique_starts:\n",
    "            words2 = words[word2_start]\n",
    "            stops2 = stops[word2_start]\n",
    "\n",
    "            # Concatenate the first word with each of the second set of words\n",
    "            if len(words2) > 0:\n",
    "                word1_copies = copy_list([word1], len(words2))\n",
    "                words2_list = [[x] for x in words2]\n",
    "                new_words.append(concatenate_lists(word1_copies, words2_list))\n",
    "                new_stops.append(stops2)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return new_words, new_stops\n",
    "            \n",
    "\n",
    "def words_stop_to_start(words, stops, unique_starts, max_stop):\n",
    "\n",
    "    updated_words = []\n",
    "    updated_stops = []\n",
    "    candidate_lines = []\n",
    "    run = True\n",
    "    while(run):\n",
    "\n",
    "        updated_words, updated_stops = concatenate_words(updated_words, updated_stops, \n",
    "                                                         words, stops, unique_starts)\n",
    "        if updated_words == [] or updated_words[-1] == []:\n",
    "            break\n",
    " \n",
    "        # Store list of words if reached max_stop\n",
    "        all_words = flatten_sublists(updated_words)\n",
    "        all_stops = flatten_list(updated_stops)\n",
    "        for istop, stop_words in enumerate(all_words):\n",
    "            stop = all_stops[istop]\n",
    "            if stop == max_stop:\n",
    "                candidate_lines.append(' '.join(all_words[istop]))\n",
    "                \n",
    "    return candidate_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db39dd",
   "metadata": {},
   "source": [
    "## Run all code on input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bd84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================================================================\n",
      "Input line:  \"Happy New Year\"\n",
      "===============================================================================\n",
      "Syllables:   4\n",
      "Stresses:    [0, 1, 0, 0, 0, 1, 0, 1, 0]\n",
      "Phonemes:    HH, AE, P, IY, N, UW, Y, IH, R\n",
      "Consonants:  HH, P, N, Y, R\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load text\n",
    "f = open(\"demo.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "\n",
    "\n",
    "verbose = True\n",
    "verbose2 = False\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    if verbose:\n",
    "        print('')\n",
    "        print('===============================================================================')\n",
    "        print('Input line:  \"{0}\"'.format(line.strip()))\n",
    "        print('===============================================================================')\n",
    "    \n",
    "    phonemes, consonants, stresses, syllables = words_to_sounds(line)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Syllables:   {0}'.format(syllables))\n",
    "        print('Stresses:    {0}'.format(stresses))\n",
    "        print('Phonemes:    {0}'.format(', '.join(phonemes)))\n",
    "        print('Consonants:  {0}'.format(', '.join(consonants)), end='\\n\\n')\n",
    "\n",
    "    phoneme_words, consonant_words = phoneme_subsets_to_words(phonemes)\n",
    "\n",
    "    if verbose2:\n",
    "        print('Phoneme words:  {0}'.format(', '.join([x[0] for x in flatten_sublists(phoneme_words)])))\n",
    "        print('Consonant words:  {0}'.format(', '.join([x[0] for x in flatten_sublists(consonant_words)])))\n",
    "\n",
    "    filtered_words_from_phonemes = filter_dictionary_words(phoneme_words, verbose=False)\n",
    "    filtered_words_from_consonants = filter_dictionary_words(consonant_words, verbose=False)\n",
    "    \n",
    "    if verbose2:\n",
    "        if filtered_words_from_phonemes != []:\n",
    "            print('Remaining words from phonemes:  {0}'.format(', '.join([x[0] for x in flatten_sublists(filtered_words_from_phonemes)])), end='\\n\\n')\n",
    "        if filtered_words_from_consonants != []:\n",
    "            print('Remaining words from consonants:  {0}'.format(', '.join([x[0] for x in flatten_sublists(filtered_words_from_consonants)])), end='\\n\\n')\n",
    "\n",
    "    words_by_start1, stops1, unique_starts1, unique_stops1, max_start1, max_stop1 = organize_words_by_start(flatten_sublists(filtered_words_from_phonemes))\n",
    "    words_by_start2, stops2, unique_starts2, unique_stops2, max_start2, max_stop2 = organize_words_by_start(flatten_sublists(filtered_words_from_consonants))\n",
    "    \n",
    "    if verbose2:\n",
    "        print('Phoneme-generated words organized by start index:')\n",
    "        print('{0}'.format(words_by_start1), end='\\n\\n')\n",
    "        print('Consonant-generated words organized by start index:')\n",
    "        print('{0}'.format(words_by_start2), end='\\n\\n')\n",
    "\n",
    "    candidate_lines1 = words_stop_to_start(words_by_start1, stops1, unique_starts1, max_stop1)    \n",
    "    candidate_lines2 = words_stop_to_start(words_by_start2, stops2, unique_starts2, max_stop2)\n",
    "    \n",
    "    if verbose:\n",
    "        if candidate_lines1 != []:\n",
    "            print('Candidate lines from phonemes:', end='\\n\\n')\n",
    "            for candidate_line1 in flatten_list(candidate_lines1):\n",
    "               print('    {0}'.format(candidate_line1), end='\\n')\n",
    "            print('')\n",
    "\n",
    "        if candidate_lines2 != []:\n",
    "            print('Candidate lines from consonants:', end='\\n\\n')\n",
    "            for candidate_line2 in flatten_list(candidate_lines2):\n",
    "               print('    {0}'.format(candidate_line2), end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeabaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
